{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6cHPrf6JYch"
   },
   "source": [
    "## Faster R-CNN(coco_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzKFE0sIsN4-"
   },
   "source": [
    "- Faster RCNN : https://pytorch.org/vision/0.9/_modules/torchvision/models/detection/faster_rcnn.html#fasterrcnn_resnet50_fpn\n",
    "\n",
    "- COCO API : https://github.com/cocodataset/cocoapi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8xEcI_9JePV"
   },
   "source": [
    "### 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA57ju6MY6b0"
   },
   "source": [
    "MS COCO 데이터셋은 약 328000장의 이미지와 80개의 클래스로 이루어져있으나, 워낙 대규모이기 때문에 개와 고양이 클래스를 소규모로 샘플링해 실습을 진행합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6902,
     "status": "ok",
     "timestamp": 1768364937747,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "qJjxcVs80diE",
    "outputId": "0b901239-da23-451d-c0f3-6ed5cb4eab87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.8.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting black>=24.10.0 (from kaggle)\n",
      "  Downloading black-25.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/86.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
      "Collecting kagglesdk<1.0,>=0.1.14 (from kaggle)\n",
      "  Downloading kagglesdk-0.1.14-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mypy>=1.15.0 (from kaggle)\n",
      "  Downloading mypy-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kaggle) (25.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
      "Collecting types-requests (from kaggle)\n",
      "  Downloading types_requests-2.32.4.20260107-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting types-tqdm (from kaggle)\n",
      "  Downloading types_tqdm-4.67.0.20250809-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black>=24.10.0->kaggle) (8.3.1)\n",
      "Collecting mypy-extensions>=0.4.3 (from black>=24.10.0->kaggle)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black>=24.10.0->kaggle)\n",
      "  Downloading pathspec-1.0.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black>=24.10.0->kaggle) (4.5.1)\n",
      "Collecting pytokens>=0.3.0 (from black>=24.10.0->kaggle)\n",
      "  Downloading pytokens-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from mypy>=1.15.0->kaggle) (4.15.0)\n",
      "Collecting librt>=0.6.2 (from mypy>=1.15.0->kaggle)\n",
      "  Downloading librt-0.7.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.12/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kaggle) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kaggle) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kaggle) (2026.1.4)\n",
      "Downloading kaggle-1.8.3-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading black-25.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kagglesdk-0.1.14-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (13.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.4.20260107-py3-none-any.whl (20 kB)\n",
      "Downloading types_tqdm-4.67.0.20250809-py3-none-any.whl (24 kB)\n",
      "Downloading librt-0.7.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (189 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading pathspec-1.0.3-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytokens-0.3.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: types-requests, pytokens, pathspec, mypy-extensions, librt, types-tqdm, mypy, kagglesdk, black, kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.7.4.5\n",
      "    Uninstalling kaggle-1.7.4.5:\n",
      "      Successfully uninstalled kaggle-1.7.4.5\n",
      "Successfully installed black-25.12.0 kaggle-1.8.3 kagglesdk-0.1.14 librt-0.7.7 mypy-1.19.1 mypy-extensions-1.1.0 pathspec-1.0.3 pytokens-0.3.0 types-requests-2.32.4.20260107 types-tqdm-4.67.0.20250809\n"
     ]
    }
   ],
   "source": [
    "# !pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2886,
     "status": "ok",
     "timestamp": 1768364940638,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "rsdjUqDx0o9E",
    "outputId": "1fe744fe-eec3-4a5e-bff2-c4f9fa9602a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'pytorch-transformer' dataset.\n",
      "Path to dataset files :  /kaggle/input/pytorch-transformer\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "path = kagglehub.dataset_download(\"s076923/pytorch-transformer\")\n",
    "print(\"Path to dataset files : \", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "executionInfo": {
     "elapsed": 96983,
     "status": "error",
     "timestamp": 1768365037622,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "2fEEPxTI1JoZ",
    "outputId": "6060a4dd-110f-4f6b-d140-192b7d867277"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/kaggle/input/pytorch-transformer/datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 18] Invalid cross-device link: '/kaggle/input/pytorch-transformer' -> './pytorch-transformer'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-151474277.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    863\u001b[0m             copytree(src, real_dst, copy_function=copy_function,\n\u001b[1;32m    864\u001b[0m                      symlinks=True)\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror, onexc, dir_fd)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;31m# Close any file descriptors still on the stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(stack, onexc)\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0monexc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n",
      "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(stack, onexc)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                 \u001b[0monexc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(stack, onexc)\u001b[0m\n\u001b[1;32m    696\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0monexc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle/input/pytorch-transformer/datasets'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.move(path, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "error",
     "timestamp": 1768365121225,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "uIe-_-SK1xYI",
    "outputId": "dfa187f6-3059-4aa3-93cf-7dcb30b0838a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/4/datasets/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3215296155.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#작업디렉토리 변경\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/4/datasets/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/4/datasets/'"
     ]
    }
   ],
   "source": [
    "#작업디렉토리 변경\n",
    "os.chdir('/content/4/datasets/')\n",
    "print(os.listdir('./'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeYMLrjaJioI"
   },
   "source": [
    "### 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUYulZzfOL9o"
   },
   "source": [
    "COCO 데이터는 \"ID\"를 기준으로 파싱을 해야 합니다.    \n",
    "images:\n",
    "\n",
    "- 각 이미지에 대한 정보가 담긴 리스트\n",
    "- 각 항목은 이미지의 고유 id, 파일명, 너비, 높이 등의 정보를 포함합니다.\n",
    "\n",
    "annotations:\n",
    "\n",
    "- 각 이미지에 대한 어노테이션 정보(예: 바운딩 박스, 라벨)를 담은 리스트\n",
    "\n",
    "categories:\n",
    "\n",
    "- 객체의 카테고리 정보를 담은 리스트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwYOOLnmN--Q"
   },
   "source": [
    "```\n",
    "{\n",
    "\t\"info\": {\n",
    "\t\t\"year\": 2021,\n",
    "\t\t\"version\": \"1.0\",\n",
    "\t\t\"description\": \"For object detection\",\n",
    "\t\t\"date_created\": \"2021\"\n",
    "\t},\n",
    "\t\"images\": [\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000001.jpg\",\n",
    "\t\t\t\"id\": 1,\n",
    "\t\t\t\"height\": 480,\n",
    "\t\t\t\"width\": 640\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000002.jpg\",\n",
    "\t\t\t\"id\": 2,\n",
    "\t\t\t\"height\": 426,\n",
    "\t\t\t\"width\": 640\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000003.jpg\",\n",
    "\t\t\t\"id\": 3,\n",
    "\t\t\t\"height\": 428,\n",
    "\t\t\t\"width\": 640\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000004.jpg\",\n",
    "\t\t\t\"id\": 4,\n",
    "\t\t\t\"height\": 425,\n",
    "\t\t\t\"width\": 640\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000005.jpg\",\n",
    "\t\t\t\"id\": 5,\n",
    "\t\t\t\"height\": 640,\n",
    "\t\t\t\"width\": 481\n",
    "\t\t}\n",
    "\t],\n",
    "\t\"licenses\": [\n",
    "\t\t{\n",
    "\t\t\t\"id\": 1,\n",
    "\t\t\t\"name\": \"GNU General Public License v3.0\",\n",
    "\t\t\t\"url\": \"https://github.com/zhiqwang/yolov5-rt-stack/blob/master/LICENSE\"\n",
    "\t\t}\n",
    "\t],\n",
    "\t\"type\": \"instances\",\n",
    "\t\"annotations\": [\n",
    "\t\t{\n",
    "\t\t\t\"segmentation\": [\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t1.0799999999999272,\n",
    "\t\t\t\t\t187.69008000000002,\n",
    "\t\t\t\t\t612.66976,\n",
    "\t\t\t\t\t187.69008000000002,\n",
    "\t\t\t\t\t612.66976,\n",
    "\t\t\t\t\t473.53008000000005,\n",
    "\t\t\t\t\t1.0799999999999272,\n",
    "\t\t\t\t\t473.53008000000005\n",
    "\t\t\t\t]\n",
    "\t\t\t],\n",
    "\t\t\t\"area\": 174816.81699840003,\n",
    "\t\t\t\"iscrowd\": 0,\n",
    "\t\t\t\"image_id\": 1,\n",
    "\t\t\t\"bbox\": [\n",
    "\t\t\t\t1.0799999999999272,\n",
    "\t\t\t\t187.69008000000002,\n",
    "\t\t\t\t611.5897600000001,\n",
    "\t\t\t\t285.84000000000003\n",
    "\t\t\t],\n",
    "\t\t\t\"category_id\": 19,\n",
    "\t\t\t\"id\": 1\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"segmentation\": [\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t311.73024,\n",
    "\t\t\t\t\t4.310159999999996,\n",
    "\t\t\t\t\t631.0102400000001,\n",
    "\t\t\t\t\t4.310159999999996,\n",
    "\t\t\t\t\t631.0102400000001,\n",
    "\t\t\t\t\t232.99032,\n",
    "\t\t\t\t\t311.73024,\n",
    "\t\t\t\t\t232.99032\n",
    "\t\t\t\t]\n",
    "\t\t\t],\n",
    "\t\t\t\"area\": 73013.00148480001,\n",
    "\t\t\t\"iscrowd\": 0,\n",
    "\t\t\t\"image_id\": 1,\n",
    "\t\t\t\"bbox\": [\n",
    "\t\t\t\t311.73024,\n",
    "\t\t\t\t4.310159999999996,\n",
    "\t\t\t\t319.28000000000003,\n",
    "\t\t\t\t228.68016\n",
    "\t\t\t],\n",
    "\t\t\t\"category_id\": 50,\n",
    "\t\t\t\"id\": 2\n",
    "\t\t},\n",
    "        ],\n",
    "\"categories\": [\n",
    "\t\t{\n",
    "\t\t\t\"id\": 1,\n",
    "\t\t\t\"name\": \"0\",\n",
    "\t\t\t\"supercategory\": \"0\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"id\": 2,\n",
    "\t\t\t\"name\": \"1\",\n",
    "\t\t\t\"supercategory\": \"1\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"id\": 3,\n",
    "\t\t\t\"name\": \"2\",\n",
    "\t\t\t\"supercategory\": \"2\"\n",
    "\t\t},\n",
    "        ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4354,
     "status": "ok",
     "timestamp": 1768365148404,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "EHvxS1BH6ZL6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768365148406,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "M3Vi4E6p6pQa"
   },
   "outputs": [],
   "source": [
    "class CustomCOCO:\n",
    "    def __init__(self, annotation_file):\n",
    "        with open(annotation_file, 'r') as f :\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        # 이미지 정보를 'id'를 키로 하는 딕셔너리로 저장합니다.\n",
    "        # 각 이미지 정보는 JSON 파일의 \"images\" 리스트에 저장되어 있습니다.\n",
    "        self.images = {img['id']: img for img in self.data.get(\"images\", [])}\n",
    "\n",
    "        # 어노테이션(annotation) 정보를 이미지 id별로 그룹화합니다.\n",
    "        # JSON 파일의 \"annotations\" 리스트를 순회하며, 각 어노테이션을 해당 이미지 id 아래에 저장합니다.\n",
    "        self.annotations = {}\n",
    "        for ann in self.data.get(\"annotations\", []):\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.annotations:\n",
    "                self.annotations[img_id] = []\n",
    "            self.annotations[img_id].append(ann)\n",
    "\n",
    "        # 카테고리 정보를 'id'를 키로 하는 딕셔너리로 저장합니다.\n",
    "        # JSON 파일의 \"categories\" 리스트에 각 카테고리 정보가 저장되어 있습니다.\n",
    "        self.cats = {cat[\"id\"]: cat for cat in self.data.get(\"categories\", [])}\n",
    "\n",
    "\n",
    "    def loadImgs(self, ids):\n",
    "        return [self.images[i] for i in ids if i in self.images]\n",
    "\n",
    "    def getAnnIds(self, imgIds):\n",
    "        ann_ids = []\n",
    "        for img_id in imgIds:\n",
    "            if img_id in self.annotations:\n",
    "                ann_ids.extend([ann['id'] for ann in self.annotations[img_id]])\n",
    "        return ann_ids\n",
    "\n",
    "    def loadAnns(self, annIds):\n",
    "        anns = []\n",
    "        for ann in self.data.get(\"annotations\", []):\n",
    "            if ann[\"id\"] in annIds:\n",
    "                anns.append(ann)\n",
    "        return anns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 37,
     "status": "error",
     "timestamp": 1768365152320,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "ML8_az3j6_mu",
    "outputId": "045823bf-4bae-4037-a410-6fcbf0e06039"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/4/datasets/coco/annotations/val_annotations.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-349351115.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/4/datasets/coco/annotations/val_annotations.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0manno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0manno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manno\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/4/datasets/coco/annotations/val_annotations.json'"
     ]
    }
   ],
   "source": [
    "with open('/content/4/datasets/coco/annotations/val_annotations.json', 'r') as f :\n",
    "            data = json.load(f)\n",
    "data['annotations'][1]['image_id']\n",
    "{anno['image_id'] : anno['category_id'] for anno in data.get(\"annotations\", [])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1768365037625,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "YibOjky_6pJI"
   },
   "outputs": [],
   "source": [
    "a = {img['id']: img[\"file_name\"] for img in data.get(\"images\", [])}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1768365194830,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "Vi9vvXd5EZo4"
   },
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root, train, transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        directory = \"train\" if train else \"val\"\n",
    "\n",
    "        #annotation 파일 경로 생성\n",
    "        annotations_file = os.path.join(root, \"annotations\", f\"{directory}_annotations.json\")\n",
    "        # 이렇게는 사용하지 X\n",
    "        # annotations_file = f\"{root}/annotations/{directory}_annotations.json\"\n",
    "        # annotations_file = f\"{root}\\\\annotations\\\\{directory}_annotations.json\"\n",
    "\n",
    "        self.coco = CustomCOCO(annotations_file)\n",
    "        self.image_path = os.path.join(root, directory)\n",
    "        self.transform = transform\n",
    "\n",
    "        # COCO 데이터셋의 카테고리 정보를 저장합니다.\n",
    "        # 0번은 배경(\"background\")으로 초기화합니다.\n",
    "        self.categories = {0 : \"background\"}\n",
    "        for cat_id, cat in self.coco.cats.items():\n",
    "            self.categories[cat_id] = cat[\"name\"]\n",
    "\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        COCO 데이터셋의 각 이미지에 대해 이미지와 해당 어노테이션 정보를 로드합니다.\n",
    "        :return: (이미지, target) 튜플들의 리스트\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for _id, img_info in self.coco.images.items():\n",
    "            file_name = img_info[\"file_name\"]\n",
    "            image_path = os.path.join(self.image_path, file_name)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            boxes = []\n",
    "            labels = []\n",
    "\n",
    "            anns = self.coco.annotations.get(_id, [])\n",
    "            for ann in anns:\n",
    "                x,y,w,h = ann['bbox']\n",
    "                # x,y,w,h --> x1,y1,x2,y2 /(x1,y1)->left up  (x2,y2)->right down\n",
    "                boxes.append([x, y, x+w, y+h])\n",
    "                labels.append(ann[\"category_id\"])\n",
    "\n",
    "            # target 딕셔너리에 이미지 id, 바운딩 박스, 라벨 정보를 텐서(tensor) 형태로 저장합니다.\n",
    "            target = {\n",
    "                'image_id': torch.LongTensor([_id]),\n",
    "                'boxes': torch.FloatTensor(boxes),\n",
    "                'labels': torch.LongTensor(labels)\n",
    "            }\n",
    "\n",
    "            data.append((image, target))\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.data[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 9957,
     "status": "ok",
     "timestamp": 1768365226102,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "ACEEV4O0LpUX"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 배치 데이터를 생성할 때, 각 배치마다 데이터를 튜플 형태로 묶어주는 함수\n",
    "# coco 데이터 셋은 이미지 내에 여러 객체 정보가 담길 수 있으므로, 데이터의 길이가 다를 수 있음.\n",
    "def collator(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(dtype=torch.float)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = COCODataset(\"/content/pytorch-transformer/datasets/coco\", train=True, transform=transform)\n",
    "test_dataset = COCODataset(\"/content/pytorch-transformer/datasets/coco\", train=False, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size =1, collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1768365037626,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "xZbJLrfbSwaU"
   },
   "outputs": [],
   "source": [
    "train_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1768365037626,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "nsWqtiUsS40-"
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upRgblXcOxVK"
   },
   "source": [
    "##### Custom Collator가 필요한 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrerZoZaPdBT"
   },
   "source": [
    "아래에 기본 collate 함수의 출력과 custom collate를 적용한 최종 출력 형태를 요약했습니다.\n",
    "\n",
    "- **기본 collate 함수 (원래 형태):**\n",
    "  - **출력:**  \n",
    "    - **이미지 텐서:** 모든 이미지가 동일한 크기라면, 자동으로 스택되어 하나의 텐서로 만들어집니다.  \n",
    "      예: `(batch_size, channels, height, width)`\n",
    "    - **타겟(라벨) 텐서/딕셔너리:** 동일한 방식으로 스택 또는 텐서 형태로 구성됩니다.\n",
    "  - **문제점:**  \n",
    "    - 이미지나 타겟의 크기가 서로 다르면 스택하는 과정에서 에러가 발생합니다.\n",
    "\n",
    "- **Custom collate 적용 후 (최종 원하는 형태):**\n",
    "  - **출력:**  \n",
    "    - **이미지 리스트:** 각 이미지가 개별적으로 리스트에 담깁니다.  \n",
    "      예: `(image1, image2, ...)`\n",
    "    - **타겟 리스트:** 각 이미지에 해당하는 타겟이 개별적으로 리스트에 담깁니다.  \n",
    "      예: `(target1, target2, ...)`\n",
    "  - **형태 요약:**  \n",
    "    - 최종 배치의 출력은 **((이미지1, 이미지2, ...), (target1, target2, ...))** 형태로 구성됩니다.\n",
    "  - **장점:**  \n",
    "    - 이미지나 타겟이 서로 다른 크기여도 그대로 유지할 수 있어, 모델 입력 전에 적절한 전처리(예: 패딩)를 적용하거나, 개별적으로 처리할 수 있습니다.\n",
    "\n",
    "이렇게 custom collate를 사용하면 서로 다른 크기의 데이터를 안전하게 배치로 묶어 모델에 입력할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1768365234926,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "U29SchcpVM1G",
    "outputId": "f5d43059-1b6c-4172-c031-340cf901b8ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [3, 300, 300] at entry 0 and [3, 400, 300] at entry 1\n",
      "배치 이미지 개수: 2\n",
      "첫 번째 이미지 크기: torch.Size([3, 300, 300])\n",
      "두 번째 이미지 크기: torch.Size([3, 400, 300])\n",
      "배치 target0 예시: {'boxes': tensor([[ 50.,  50., 200., 200.]]), 'labels': tensor([1])}\n",
      "배치 target1 예시: {'boxes': tensor([[100., 100., 300., 350.],\n",
      "        [ 30.,  20., 200., 300.]]), 'labels': tensor([2, 3])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "##### 샘플 데이터 생성 ######\n",
    "# 두 개의 샘플 이미지 (각각 다른 크기의 텐서)\n",
    "image1 = torch.randn(3,300,300)\n",
    "target1 = {\n",
    "    \"boxes\" : torch.tensor([[50,50,200,200]], dtype=torch.float32),\n",
    "    \"labels\" : torch.tensor([1])\n",
    "}\n",
    "\n",
    "image2 = torch.randn(3,400,300)\n",
    "target2 = {\n",
    "    \"boxes\" : torch.tensor([[100,100,300,350],[30,20,200,300]], dtype=torch.float32),\n",
    "    \"labels\": torch.tensor([2,3])\n",
    "}\n",
    "\n",
    "sample_data = [(image1, target1), (image2, target2)]\n",
    "\n",
    "##### 기본 collate 함수 #####\n",
    "loader_without_collator = DataLoader(sample_data, batch_size=2)\n",
    "\n",
    "try:\n",
    "    for batch in loader_without_collator:\n",
    "        images, targets = batch\n",
    "        print(\"Batch images shape : \", images.shape)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # stack expects each tensor to be equal size,\n",
    "    # but got [3, 300, 300] at entry 0 and [3, 400, 400] at entry 1\n",
    "    # stack expects each tensor to be equal size,\n",
    "    # but got [1, 4] at entry 0 and [2, 4] at entry 1\n",
    "\n",
    "\n",
    "##### Custom Collate 함수 #####\n",
    "\n",
    "def collator(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "loader_with_collator = DataLoader(sample_data, batch_size=2, collate_fn=collator)\n",
    "\n",
    "for batch in loader_with_collator:\n",
    "    images, targets = batch\n",
    "    print(\"배치 이미지 개수:\", len(images))\n",
    "    print(\"첫 번째 이미지 크기:\", images[0].shape)\n",
    "    print(\"두 번째 이미지 크기:\", images[1].shape)\n",
    "    print(\"배치 target0 예시:\", targets[0])\n",
    "    print(\"배치 target1 예시:\", targets[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1768365037627,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "Y3KKjdB7Yy-9"
   },
   "outputs": [],
   "source": [
    "a = torch.randn(3,200,200)\n",
    "b = torch.randn(3,200,200)\n",
    "\n",
    "a_t = torch.tensor([[0],[2],[3]])\n",
    "b_t = torch.tensor([[1]])\n",
    "\n",
    "print(torch.stack((a,b)).size())\n",
    "print(torch.stack((a_t,b_t)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1768365238316,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "ZpKiFx6Oqxm1"
   },
   "outputs": [],
   "source": [
    "#@title 모델링\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import ops\n",
    "from torchvision.models.detection import rpn\n",
    "from torchvision.models.detection import FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 2628,
     "status": "ok",
     "timestamp": 1768365798235,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "mg6js5Byqy9b"
   },
   "outputs": [],
   "source": [
    "backbone = models.vgg16(weights=\"VGG16_Weights.IMAGENET1K_V1\").features\n",
    "backbone.out_channels = 512\n",
    "\n",
    "#RPN에서 사용할 앵커 생성기 설정\n",
    "anchor_generator = rpn.AnchorGenerator(\n",
    "    sizes=((32,64,128,256,512),),\n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),)\n",
    ")\n",
    "\n",
    "roi_pooler = ops.MultiScaleRoIAlign(\n",
    "    featmap_names =['0'],\n",
    "    output_size = (7,7),\n",
    "    sampling_ratio = 2\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Faster R-CNN 모델을 생성합니다.\n",
    "# - backbone: 특징 추출기 (VGG16 사용)\n",
    "# - num_classes: 검출할 클래스 수 (배경 포함; 여기서는 3개로 설정)\n",
    "# - rpn_anchor_generator: 위에서 설정한 앵커 생성기\n",
    "# - box_roi_pool: 위에서 설정한 ROI 풀링 모듈\n",
    "\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=3,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1768365250919,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "xncsyyt7qyxJ"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 185154,
     "status": "error",
     "timestamp": 1768365986039,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "lbobHw5yqyl2",
    "outputId": "075560ef-7ca5-47ca-fe76-e02ca20d041f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5:  16%|█▋        | 100/607 [03:05<15:38,  1.85s/it, loss=1.874]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1389700192.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{losses.item():.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_dataloader, desc=f\"{epoch+1}/{num_epochs}\")\n",
    "    for images, targets in train_bar:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        train_bar.set_postfix(loss = f\"{losses.item():.3f}\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    avg_train_loss = total_loss  / len(train_dataloader)\n",
    "    print(f\" {epoch+1:2d}, Ave Train Loss {avg_train_loss:.3f}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxX62JbSyoJl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1nP__Ghdj53P2PZ_MGPn_Mr98zH5hLL0N"
    },
    "executionInfo": {
     "elapsed": 2294,
     "status": "ok",
     "timestamp": 1768365461747,
     "user": {
      "displayName": "윤석민",
      "userId": "11344447928685279078"
     },
     "user_tz": -540
    },
    "id": "CZzx-bGUqyaP",
    "outputId": "fa3c9ec9-d458-4d69-8b0a-bae9bcb2c4d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 테스트 및 시각화 관련 코드 ###\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# 바운딩 박스를 그림으로 표시하는 함수\n",
    "def draw_bbox(ax, box, text, color):\n",
    "    \"\"\"\n",
    "    바운딩 박스를 이미지에 그려주는 함수.\n",
    "    :param ax: matplotlib Axes 객체\n",
    "    :param box: 바운딩 박스 좌표 (x_min, y_min, x_max, y_max)\n",
    "    :param text: 바운딩 박스 위에 표시할 텍스트\n",
    "    :param color: 바운딩 박스와 텍스트의 색상\n",
    "    \"\"\"\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle(\n",
    "            xy=(box[0], box[1]),\n",
    "            width=box[2] - box[0],\n",
    "            height=box[3] - box[1],\n",
    "            fill=False,\n",
    "            edgecolor=color,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    )\n",
    "    ax.annotate(\n",
    "        text=text,\n",
    "        xy=(box[0] - 5, box[1] - 5),\n",
    "        color=color,\n",
    "        weight=\"bold\",\n",
    "        fontsize=13,\n",
    "    )\n",
    "\n",
    "num_vis = 0\n",
    "threshold = 0.5  # 예측 점수 임계값: 이 값 이상인 예측만 시각화에 사용\n",
    "categories = test_dataset.categories  # COCO 데이터셋의 카테고리 정보 사용\n",
    "with torch.no_grad():  # 평가 시에는 기울기 계산을 하지 않습니다.\n",
    "    model.eval()       # 모델을 평가 모드로 전환\n",
    "    for images, targets in test_dataloader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        outputs = model(images)  # 모델로부터 예측 결과 생성\n",
    "\n",
    "        # 예측 결과에서 바운딩 박스, 라벨, 점수를 CPU의 numpy 배열로 변환\n",
    "        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n",
    "        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n",
    "        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n",
    "\n",
    "        # 임계값 이상의 예측만 선택\n",
    "        boxes = boxes[scores >= threshold].astype(np.int32)\n",
    "        labels = labels[scores >= threshold]\n",
    "        scores = scores[scores >= threshold]\n",
    "\n",
    "        # 시각화를 위한 플롯 설정 (8x8 크기의 그림)\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        plt.imshow(to_pil_image(images[0]))  # 첫 번째 이미지를 PIL 이미지로 변환하여 출력\n",
    "\n",
    "        # 예측 결과(빨간색 박스) 시각화\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")\n",
    "\n",
    "        # 정답 어노테이션(파란색 박스) 시각화\n",
    "        tboxes = targets[0][\"boxes\"].numpy()\n",
    "        tlabels = targets[0][\"labels\"].numpy()\n",
    "        for box, label in zip(tboxes, tlabels):\n",
    "            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")\n",
    "\n",
    "        plt.show()  # 시각화 결과 출력\n",
    "        num_vis += 1\n",
    "        if num_vis == 5:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNAjOcGVlc1WCEMnYzmZsFR",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
