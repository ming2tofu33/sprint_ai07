{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2f134c",
   "metadata": {},
   "source": [
    "# 텍스트 데이터와 임베딩의 발전사\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 통계 및 빈도 기반 시대 (1970년대 ~ 2000년대 초반)\n",
    "\n",
    "이 시기에는 단어의 의미보다는 문서 내 등장 빈도를 기반으로 텍스트를 벡터화했습니다.\n",
    "\n",
    "###  TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- **개념**: 1972년 Karen Spärck Jones가 제안한 IDF 개념에서 발전한 기법으로, 단어의 중요도를 평가하는 통계적 가중치입니다.\n",
    "  - 특정 문서에 자주 나타나는 단어(TF)에 높은 가중치를 부여\n",
    "  - 여러 문서에 공통적으로 등장하는 단어(IDF)에는 낮은 가중치를 부여\n",
    "- **공헌**: 정보 검색 분야의 초석을 다졌으며, 문서의 핵심 키워드 추출 및 문서 간 유사도 계산에 널리 사용되었습니다.\n",
    "\n",
    "###  Bag-of-Words (BoW)\n",
    "- **개념**: 문서를 단어들의 순서를 무시한 '가방'으로 간주하고, 등장 횟수를 벡터화하여 표현\n",
    "- **공헌**: 구현이 직관적이고 간단하여, 초기 NLP 문제(텍스트 분류, 감성 분석 등)에 기본 모델로 활용되었습니다.\n",
    "\n",
    "---\n",
    "\n",
    "##  2. 정적 단어 임베딩 시대 (2013년 ~ 2017년)\n",
    "\n",
    "신경망을 이용해 단어의 의미 자체를 저차원의 밀집 벡터(Dense Vector)에 담아내는 단어 임베딩 기술이 등장하며 NLP 분야에 혁신을 가져왔습니다.\n",
    "\n",
    "###  Word2Vec (2013)\n",
    "- **논문**: *Efficient Estimation of Word Representations in Vector Space* (Mikolov et al.)\n",
    "- **핵심 개념**:\n",
    "  - \"단어의 의미는 주변 단어에 의해 결정된다\"는 분포 가설에 기반\n",
    "  - CBOW (주변 단어 → 중심 단어) / Skip-gram (중심 단어 → 주변 단어)\n",
    "- **공헌**:\n",
    "  - '왕' - '남자' + '여자' ≈ '여왕' 같은 벡터 연산 가능\n",
    "  - 단어 의미를 벡터 공간에 투영하며 NLP 패러다임 전환\n",
    "\n",
    "###  GloVe (2014)\n",
    "- **논문**: *GloVe: Global Vectors for Word Representation* (Pennington et al.)\n",
    "- **핵심 개념**:\n",
    "  - 단어 동시 등장 행렬(co-occurrence matrix) 기반 통계적 모델\n",
    "  - Word2Vec의 예측 기반 학습과 통계 기반 방법을 융합\n",
    "- **공헌**: 더 안정적이고 빠르게 학습 가능, Word2Vec과 함께 대표적인 정적 임베딩 기법\n",
    "\n",
    "###  FastText (2017)\n",
    "- **논문**: *Enriching Word Vectors with Subword Information* (Bojanowski et al.)\n",
    "- **핵심 개념**:\n",
    "  - 단어를 n-gram 문자 단위로 분해해 학습\n",
    "  - 예: 'apple' → 'app', 'ppl', 'ple'\n",
    "- **공헌**:\n",
    "  - 사전에 없는 단어(OOV)에 강건함\n",
    "  - 형태소 유사 단어 간 유의미한 임베딩 공유\n",
    "\n",
    "---\n",
    "\n",
    "##  3. 문맥적 임베딩 시대 (2018년 ~ 현재)\n",
    "\n",
    "같은 단어라도 문맥에 따라 의미가 달라짐을 반영하여, 문장 전체 정보를 바탕으로 단어 임베딩을 동적으로 생성하는 모델들이 등장했습니다.\n",
    "\n",
    "###  ELMo (2018)\n",
    "- **논문**: *Deep contextualized word representations* (Peters et al.)\n",
    "- **핵심 개념**:\n",
    "  - BiLSTM으로 문맥 양방향 학습\n",
    "  - '사과'가 '과일'인지 '사과하다'인지 문맥에 따라 벡터 다르게 생성\n",
    "- **공헌**:\n",
    "  - 문맥적 임베딩 시대의 개막\n",
    "  - 이후 Transformer 모델에 강한 영향\n",
    "\n",
    "###  Transformer & Attention (2017)\n",
    "- **논문**: *Attention Is All You Need* (Vaswani et al.)\n",
    "- **핵심 개념**:\n",
    "  - RNN 없이 Self-Attention만으로 단어 간 관계를 파악\n",
    "  - 병렬 처리 및 장기 의존성 문제 해결\n",
    "- **공헌**:\n",
    "  - BERT, GPT 등 현대 LLM의 기반 아키텍처로 자리잡음\n",
    "\n",
    "###  BERT (2018)\n",
    "- **논문**: *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding* (Devlin et al.)\n",
    "- **핵심 개념**:\n",
    "  - Transformer Encoder 기반\n",
    "  - MLM (Masked Language Modeling)으로 양방향 문맥 학습\n",
    "- **공헌**:\n",
    "  - 사전학습 → 미세조정(Pre-training → Fine-tuning) 패러다임 정립\n",
    "  - 대부분 NLP 태스크에서 SOTA 달성\n",
    "\n",
    "###  GPT 시리즈 (2018~)\n",
    "- **논문**:\n",
    "  - GPT-1: *Improving Language Understanding by Generative Pre-Training*\n",
    "  - GPT-2: *Language Models are Unsupervised Multitask Learners*\n",
    "  - GPT-3: *Language Models are Few-Shot Learners*\n",
    "- **핵심 개념**:\n",
    "  - Transformer Decoder 기반\n",
    "  - 다음 단어 예측 방식, 대규모 사전학습 데이터 활용\n",
    "- **공헌**:\n",
    "  - LLM 시대의 개막\n",
    "  - ChatGPT 등 다양한 AI 서비스의 기반\n",
    "\n",
    "---\n",
    "\n",
    "##  4. 최신 임베딩 및 특화 모델 시대\n",
    "\n",
    "문장 의미 임베딩 및 태스크 특화 임베딩 기술이 활발하게 연구되고 있습니다.\n",
    "\n",
    "###  Sentence-BERT (2019)\n",
    "- **논문**: *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks* (Reimers & Gurevych)\n",
    "- **핵심 개념**:\n",
    "  - Siamese 구조를 사용해 문장 유사도 학습\n",
    "  - BERT 출력값의 cosine similarity를 바로 활용\n",
    "- **공헌**:\n",
    "  - 문장 유사도 계산 및 검색에 특화된 효율적 임베딩 제공\n",
    "\n",
    "###  SimCSE (2021)\n",
    "- **논문**: *SimCSE: Simple Contrastive Learning of Sentence Embeddings* (Gao et al.)\n",
    "- **핵심 개념**:\n",
    "  - 동일 문장을 Dropout만 다르게 적용한 두 개로 Contrastive Learning 수행\n",
    "- **공헌**:\n",
    "  - 비지도 학습만으로도 뛰어난 문장 임베딩 성능 확보\n",
    "  - 문장 표현 학습의 새 방향성 제시\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8538c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6ebdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>rat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>euge****</td>\n",
       "      <td>22.09.16.</td>\n",
       "      <td>새부리 KF94 마스크를 검색하니 네이버 랭킹 상위에 뜨고 가격도 착해서 호기심에 ...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemm****</td>\n",
       "      <td>22.05.03.</td>\n",
       "      <td>컬러 KF94 마스크를 너무나도 기다렸어요\\n예쁜 컬러에 벌크포장과 저렴하면서 퀄리...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemm****</td>\n",
       "      <td>22.05.06.</td>\n",
       "      <td>컬러 KF94 마스크를 너무나도 기다렸어요\\n세상 어디에도 없는 라이트실버 H워월V...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jina****</td>\n",
       "      <td>22.07.22.</td>\n",
       "      <td>라이브 보고 방송하는언니 쓴거 넘 예뻐서 100개 주문했는데 가로사이즈가 저한테 좀...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pim4****</td>\n",
       "      <td>22.10.21.</td>\n",
       "      <td>고민고민하다 디럭스 실버를 주문했는데\\n진짜 이쁩니다. 과하지 않지만 평범하지도 않...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       date                                               text  rat\n",
       "0  euge****  22.09.16.  새부리 KF94 마스크를 검색하니 네이버 랭킹 상위에 뜨고 가격도 착해서 호기심에 ...  평점5\n",
       "1  gemm****  22.05.03.  컬러 KF94 마스크를 너무나도 기다렸어요\\n예쁜 컬러에 벌크포장과 저렴하면서 퀄리...  평점5\n",
       "2  gemm****  22.05.06.  컬러 KF94 마스크를 너무나도 기다렸어요\\n세상 어디에도 없는 라이트실버 H워월V...  평점5\n",
       "3  jina****  22.07.22.  라이브 보고 방송하는언니 쓴거 넘 예뻐서 100개 주문했는데 가로사이즈가 저한테 좀...  평점5\n",
       "4  pim4****  22.10.21.  고민고민하다 디럭스 실버를 주문했는데\\n진짜 이쁩니다. 과하지 않지만 평범하지도 않...  평점5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./content/nreview_mask.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248bfcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# text cleansing\n",
    "# pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf44477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>rat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>euge****</td>\n",
       "      <td>22.09.16.</td>\n",
       "      <td>새부리 KF94 마스크를 검색하니 네이버 랭킹 상위에 뜨고 가격도 착해서 호기심에 ...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemm****</td>\n",
       "      <td>22.05.03.</td>\n",
       "      <td>컬러 KF94 마스크를 너무나도 기다렸어요\\n예쁜 컬러에 벌크포장과 저렴하면서 퀄리...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemm****</td>\n",
       "      <td>22.05.06.</td>\n",
       "      <td>컬러 KF94 마스크를 너무나도 기다렸어요\\n세상 어디에도 없는 라이트실버 H워월V...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jina****</td>\n",
       "      <td>22.07.22.</td>\n",
       "      <td>라이브 보고 방송하는언니 쓴거 넘 예뻐서 100개 주문했는데 가로사이즈가 저한테 좀...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pim4****</td>\n",
       "      <td>22.10.21.</td>\n",
       "      <td>고민고민하다 디럭스 실버를 주문했는데\\n진짜 이쁩니다. 과하지 않지만 평범하지도 않...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2175</th>\n",
       "      <td>goon****</td>\n",
       "      <td>22.05.10.</td>\n",
       "      <td>4중구조인 거에 비해 얇아요 그리고 귀끈이 진짜 편합니다 A사는 귀는 편하나 딱맞는...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2176</th>\n",
       "      <td>bboy****</td>\n",
       "      <td>23.04.04.</td>\n",
       "      <td>가격 싸서 좋아요 하지만\\n흰색 마스크를 제외한 컬러가 들어간 마스크는\\n코 부분 ...</td>\n",
       "      <td>평점4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>0019****</td>\n",
       "      <td>22.08.13.</td>\n",
       "      <td>우순 엄청큰 봉투가 와서 놀랏고 ㅋㅋㅋ\\n마구마구 넣으신듯한... 상자에 구겨져들어...</td>\n",
       "      <td>평점3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>leej****</td>\n",
       "      <td>22.08.21.</td>\n",
       "      <td>기존에 에코브리즈 다른 마스크보다 우선 시원하고 크기도 큼직해서 개인적으로 기미라인...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2179</th>\n",
       "      <td>buru****</td>\n",
       "      <td>22.11.17.</td>\n",
       "      <td>지난번 라방때 에코브리즈 중형 사보고 가볍고 숨쉬기 편한 질감은 만족스러운데 살짝 ...</td>\n",
       "      <td>평점5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id       date                                               text  \\\n",
       "0     euge****  22.09.16.  새부리 KF94 마스크를 검색하니 네이버 랭킹 상위에 뜨고 가격도 착해서 호기심에 ...   \n",
       "1     gemm****  22.05.03.  컬러 KF94 마스크를 너무나도 기다렸어요\\n예쁜 컬러에 벌크포장과 저렴하면서 퀄리...   \n",
       "2     gemm****  22.05.06.  컬러 KF94 마스크를 너무나도 기다렸어요\\n세상 어디에도 없는 라이트실버 H워월V...   \n",
       "3     jina****  22.07.22.  라이브 보고 방송하는언니 쓴거 넘 예뻐서 100개 주문했는데 가로사이즈가 저한테 좀...   \n",
       "4     pim4****  22.10.21.  고민고민하다 디럭스 실버를 주문했는데\\n진짜 이쁩니다. 과하지 않지만 평범하지도 않...   \n",
       "...        ...        ...                                                ...   \n",
       "2175  goon****  22.05.10.  4중구조인 거에 비해 얇아요 그리고 귀끈이 진짜 편합니다 A사는 귀는 편하나 딱맞는...   \n",
       "2176  bboy****  23.04.04.  가격 싸서 좋아요 하지만\\n흰색 마스크를 제외한 컬러가 들어간 마스크는\\n코 부분 ...   \n",
       "2177  0019****  22.08.13.  우순 엄청큰 봉투가 와서 놀랏고 ㅋㅋㅋ\\n마구마구 넣으신듯한... 상자에 구겨져들어...   \n",
       "2178  leej****  22.08.21.  기존에 에코브리즈 다른 마스크보다 우선 시원하고 크기도 큼직해서 개인적으로 기미라인...   \n",
       "2179  buru****  22.11.17.  지난번 라방때 에코브리즈 중형 사보고 가볍고 숨쉬기 편한 질감은 만족스러운데 살짝 ...   \n",
       "\n",
       "      rat  \n",
       "0     평점5  \n",
       "1     평점5  \n",
       "2     평점5  \n",
       "3     평점5  \n",
       "4     평점5  \n",
       "...   ...  \n",
       "2175  평점5  \n",
       "2176  평점4  \n",
       "2177  평점3  \n",
       "2178  평점5  \n",
       "2179  평점5  \n",
       "\n",
       "[2180 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f809e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! This is NLP. 자연어 처리를 배워봅시다! 123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d7506d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world! This is NLP. 자연어 처리를 배워봅시다! 123'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961b43f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fdf69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89c3c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, world! this is nlp. 자연어 처리를 배워봅시다! 123'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8271f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'world!', 'This', 'is', 'NLP.', '자연어', '처리를', '배워봅시다!', '123']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39b31228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a1d18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'NLP' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e66cb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'vision' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dc17116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.index('NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5de03300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world! This is 자연어처리. 자연어 처리를 배워봅시다! 123'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.replace('NLP', '자연어처리')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c57d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb, .py\n",
    "def plus1(a, b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b90f686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus1(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf2cc3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'world!', 'This', 'is', '자연어처리.', '자연어', '처리를', '배워봅시다!', '123']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e67ca260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words[0]), len(words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a27b837a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.333333333333333\n"
     ]
    }
   ],
   "source": [
    "sum_ = 0\n",
    "for w in words:\n",
    "    sum_ += len(w)\n",
    "    # print(len(w))\n",
    "\n",
    "print(sum_ / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1df0b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stats(text):\n",
    "    # return 전체 문자 수 , 단어 수, 평균 단어들의 길이\n",
    "    words = text.split()\n",
    "    cnt_chr = len(text)\n",
    "    cnt_word = len(words)\n",
    "    avg_word_len = round(sum(len(s) for s in words) / len(words), 2)\n",
    "    return {'chr_cnt': cnt_chr,\n",
    "            'word_cnt': cnt_word,\n",
    "            'avg_word_len': avg_word_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ce991c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chr_cnt': 43, 'word_cnt': 9, 'avg_word_len': 3.89}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'The quick brown fox jumps over the lazy dog'\n",
    "text_stats(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daf6807d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "413ad619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "197fa685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(sample.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06f0d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"U.S.A is a country with 328 million people as of 2020!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb05e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u.s.a is a country with 328 million people as of 2020!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lower = text.lower()\n",
    "text_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c57f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re   # regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f4e8b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usa is a country with  million people as of '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean = re.sub(r'[^a-zA-Z가-힣\\s]', '', text_lower)\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e520cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ a-z A-Z 가-힣 \\s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ffc3746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usa is a country with million people as of '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean = re.sub(r'\\s+', ' ', text_clean)\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49864323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub(r'^a-zA-Z가-힣', '', text)\n",
    "# re.sub(r'\\d+', '', text)\n",
    "# re.sub(r'\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "664886a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"문의 : test@email.com, https://shop.com 참고 (Tel: 010-1234-5678) 좋은 상품!! 가격: 15000!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "87615e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문의 : , https://shop.com 참고 (Tel: 010-1234-5678) 좋은 상품!! 가격: 15000!!'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1 = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', text)\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b7e979bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문의 : ,  참고 (Tel: 010-1234-5678) 좋은 상품!! 가격: 15000!!'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = re.sub(r'http?://\\S+','', step1)\n",
    "step2 = re.sub(r'https?://\\S+','', step2)\n",
    "step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5bdbde3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문의 : ,  참고 (Tel: ) 좋은 상품!! 가격: 15000!!'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = re.sub(r'\\d{2,3}-\\d{3,4}-\\d{4}', '', step2)\n",
    "step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3e397e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문의    참고 Tel  좋은 상품 가격 15000'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step4 = re.sub(r'[^a-zA-Z가-힣0-9\\s]', '', step3)\n",
    "step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "edb4c6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문의 참고 Tel 좋은 상품 가격 15000'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step5 = re.sub(r'\\s+', ' ', step4)\n",
    "step5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "40f1a781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcde'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =  '     abcde    '\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a2144712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     abcde'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =  '     abcde    '\n",
    "text.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b1095dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     abcde    '"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'<[^>]>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7cd8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_numbers=True):\n",
    "    # 이메일 주소 제거\n",
    "    # url 제거\n",
    "    # 전화번호 제거\n",
    "    # remove_numbers 가 True 이면 숫자도 제거\n",
    "    # 한글, 영문, 공백만 유지\n",
    "    text = re.sub(r'[a-zA-Z0-9._%+0-]+@[a-zA-Z0-9.-]+\\.[a-zA-z]{2,}', '', text)\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z가-힣0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', ' ', text) if remove_numbers else re.sub(r'[^a-zA-Z가-힣0-9\\s]', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0309a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"문의 : test@email.com, https://shop.com 참고 (Tel: 010-1234-5678) 좋은 상품!! 가격: 15000!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a05cac96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문의    참고 Tel   좋은 상품 가격  '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3722291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 자주 쓰는 패턴들\n",
    "_EMAIL_RE = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
    "_URL_RE   = re.compile(r'\\b(?:https?://|www\\.)\\S+\\b', re.IGNORECASE)\n",
    "\n",
    "# 한국 전화번호(010-1234-5678, 02-123-4567 등) + 국제형 일부 커버\n",
    "_PHONE_RE = re.compile(\n",
    "    r'(?:(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{3,4}[-.\\s]?\\d{4})'\n",
    ")\n",
    "\n",
    "# 한글/영문/공백만 남기기\n",
    "_KEEP_KO_EN_SPACE_RE = re.compile(r'[^A-Za-z가-힣\\s]')\n",
    "\n",
    "def clean_text(text, remove_numbers=True):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    # 1) 이메일 제거\n",
    "    text = _EMAIL_RE.sub(\" \", text)\n",
    "\n",
    "    # 2) URL 제거\n",
    "    text = _URL_RE.sub(\" \", text)\n",
    "\n",
    "    # 3) 전화번호 제거\n",
    "    text = _PHONE_RE.sub(\" \", text)\n",
    "\n",
    "    # 4) 숫자 제거 옵션\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "    # 5) 한글, 영문, 공백만 유지\n",
    "    text = _KEEP_KO_EN_SPACE_RE.sub(\" \", text)\n",
    "\n",
    "    # 6) 공백 정리\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ff0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk konlpy kiwipiepy python-mecab-ko sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01d3f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2457d9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41855460",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Dr. Smith went to Washington D.C. He didn't arrive until 3:30 p.m.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6520d7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Smith',\n",
       " 'went',\n",
       " 'to',\n",
       " 'Washington',\n",
       " 'D.C.',\n",
       " 'He',\n",
       " \"didn't\",\n",
       " 'arrive',\n",
       " 'until',\n",
       " '3:30',\n",
       " 'p.m.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8457f6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Smith',\n",
       " 'went',\n",
       " 'to',\n",
       " 'Washington',\n",
       " 'D.C',\n",
       " '.',\n",
       " 'He',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'arrive',\n",
       " 'until',\n",
       " '3:30',\n",
       " 'p.m',\n",
       " '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d04aac6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr. Smith went to Washington D.C.', \"He didn't arrive until 3:30 p.m.\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40cdd8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_tokenize(text):\n",
    "#     text = text.lower()\n",
    "#     for ~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf547358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 : a the is at on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bfe58474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2bdc949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0f8fa3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The movie was not great but it was not terrible either\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7e8f2b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'movie',\n",
       " 'was',\n",
       " 'not',\n",
       " 'great',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'not',\n",
       " 'terrible',\n",
       " 'either']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b253b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b93a51bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie', 'great', 'terrible', 'either']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = [w for w in tokens if w not in stop_words and w.isalpha()]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2de16004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered = []\n",
    "# for w in tokens:\n",
    "#     if w not in stop_words and w.isalpha():\n",
    "#         filtered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0b4175d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연어', '처리는', '인공지능의', '핵심', '분야', '중', '하나이다']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_stopwords = {'은', '는', '이', '가', '을', '를', '에', '에서', '의', '도', '로', '으로'}\n",
    "korean_text = \"자연어 처리는 인공지능의 핵심 분야 중 하나이다\"\n",
    "tokens = korean_text.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "469fcc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5926bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['studies', 'studying', 'studied', 'better', 'running', 'ran', 'feet', 'wolves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5eecb5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본:studies, stemmer:studi, lemmar word:study, lemmar v:study\n",
      "원본:studying, stemmer:studi, lemmar word:studying, lemmar v:study\n",
      "원본:studied, stemmer:studi, lemmar word:studied, lemmar v:study\n",
      "원본:better, stemmer:better, lemmar word:better, lemmar v:better\n",
      "원본:running, stemmer:run, lemmar word:running, lemmar v:run\n",
      "원본:ran, stemmer:ran, lemmar word:ran, lemmar v:run\n",
      "원본:feet, stemmer:feet, lemmar word:foot, lemmar v:feet\n",
      "원본:wolves, stemmer:wolv, lemmar word:wolf, lemmar v:wolves\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(f\"원본:{w}, stemmer:{stemmer.stem(w)}, lemmar word:{lemmatizer.lemmatize(w)}, lemmar v:{lemmatizer.lemmatize(w, pos='v')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16667db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'studi'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "67585c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Stem</th>\n",
       "      <th>Lemma_noun</th>\n",
       "      <th>Lemma_verb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>studies</td>\n",
       "      <td>studi</td>\n",
       "      <td>study</td>\n",
       "      <td>study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studying</td>\n",
       "      <td>studi</td>\n",
       "      <td>studying</td>\n",
       "      <td>study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>studied</td>\n",
       "      <td>studi</td>\n",
       "      <td>studied</td>\n",
       "      <td>study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ran</td>\n",
       "      <td>ran</td>\n",
       "      <td>ran</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feet</td>\n",
       "      <td>feet</td>\n",
       "      <td>foot</td>\n",
       "      <td>feet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wolves</td>\n",
       "      <td>wolv</td>\n",
       "      <td>wolf</td>\n",
       "      <td>wolves</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word    Stem Lemma_noun Lemma_verb\n",
       "0   studies   studi      study      study\n",
       "1  studying   studi   studying      study\n",
       "2   studied   studi    studied      study\n",
       "3    better  better     better     better\n",
       "4   running     run    running        run\n",
       "5       ran     ran        ran        run\n",
       "6      feet    feet       foot       feet\n",
       "7    wolves    wolv       wolf     wolves"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization과 stemming의 결과를 비교하는 표를 만들어 반환하는 함수 (이용 라이브러리 : 판다스)\n",
    "def compare_normalization(words):    \n",
    "    data = []\n",
    "    for w in words:\n",
    "        stem = stemmer.stem(w)\n",
    "        lemma_w = lemmatizer.lemmatize(w)\n",
    "        lemma_v = lemmatizer.lemmatize(w, pos='v')\n",
    "        data.append({'Word': w, 'Stem': stem, 'Lemma_noun': lemma_w, 'Lemma_verb': lemma_v})\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "compare_normalization(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06c16f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konlpy\n",
    "# mecab-ko\n",
    "# kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a93d0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dd26320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '자연어처리는 인공지능의 핵심 분야 중 하나입니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6598ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "915ff016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연어', '처리', '는', '인공', '지능', '의', '핵심', '분야', '중', '하나', '입니다', '.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0784a152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연어', '처리', '인공', '지능', '핵심', '분야', '중', '하나']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dde03bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('자연어', 'Noun'),\n",
       " ('처리', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('인공', 'Noun'),\n",
       " ('지능', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('핵심', 'Noun'),\n",
       " ('분야', 'Noun'),\n",
       " ('중', 'Noun'),\n",
       " ('하나', 'Noun'),\n",
       " ('입니다', 'Adjective'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos(text, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fdf79a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b6d3b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='자연어 처리', tag='NNP', start=0, len=5),\n",
       " Token(form='는', tag='JX', start=5, len=1),\n",
       " Token(form='인공', tag='NNG', start=7, len=2),\n",
       " Token(form='지능', tag='NNG', start=9, len=2),\n",
       " Token(form='의', tag='JKG', start=11, len=1),\n",
       " Token(form='핵심', tag='NNG', start=13, len=2),\n",
       " Token(form='분야', tag='NNG', start=16, len=2),\n",
       " Token(form='중', tag='NNB', start=19, len=1),\n",
       " Token(form='하나', tag='NR', start=21, len=2),\n",
       " Token(form='이', tag='VCP', start=23, len=1),\n",
       " Token(form='ᆸ니다', tag='EF', start=23, len=3),\n",
       " Token(form='.', tag='SF', start=26, len=1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = kiwi.tokenize(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f900b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mecab import MeCab\n",
    "mecab = MeCab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8ddb989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('자연어', 'NNG'),\n",
       " ('처리', 'NNG'),\n",
       " ('는', 'JX'),\n",
       " ('인공지능', 'NNP'),\n",
       " ('의', 'JKG'),\n",
       " ('핵심', 'NNG'),\n",
       " ('분야', 'NNG'),\n",
       " ('중', 'NNB'),\n",
       " ('하나', 'NR'),\n",
       " ('입니다', 'VCP+EF'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = mecab.pos(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT : ['아버지', '가방', '에', '들어가신다']\n",
      "MeCab : ['아버지', '가', '방', '에', '들어가', '신다']\n",
      "Kiwi : ['아버지', '가', '방', '에', '들어가', '시', 'ᆫ다']\n"
     ]
    }
   ],
   "source": [
    "text = '아버지가방에들어가신다'\n",
    "\n",
    "print(f'OKT : {okt.morphs(text)}')   # 명사만 추출\n",
    "print(f'MeCab : {[w for w, _ in mecab.pos(text)]}')    # w == 형태소, _ == 품사(태깅)\n",
    "print(f'Kiwi : {[t.form for t in kiwi.tokenize(text)]}')    # form ==형태소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65b60207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('방', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('들어가', 'VV'),\n",
       " ('신다', 'EP+EC')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "16e58b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지', '가', '방', '에', '들어가', '신다']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_mecab = []\n",
    "for w, _ in mecab.pos(text):\n",
    "    result_mecab.append(w)\n",
    "\n",
    "result_mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e10670dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Token(form='아버지', tag='NNG', start=0, len=3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiwi.tokenize(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "105aa1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('아버지', 'NNG')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiwi.tokenize(text)[0].form, kiwi.tokenize(text)[0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eafa8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_analyze(text):\n",
    "    data = []\n",
    "    okt_morphs = okt.morphs(text)\n",
    "    okt_nouns = okt.nouns(text)\n",
    "    \n",
    "    mecab_result = mecab.pos(text)\n",
    "    mecab_morphs = [w for w, t in mecab_result]\n",
    "    mecab_nouns = [w for w, t in mecab_result if t.startswith('NN')]\n",
    "    \n",
    "    kiwi_results = kiwi.tokenize(text)\n",
    "    kiwi_morphs = [t.form for t in kiwi_results]\n",
    "    kiwi_nouns = [t.form for t in kiwi_results if t.tag == 'NNG']\n",
    "    \n",
    "    data.append({'analyzer': 'okt', 'morphs': okt_morphs, 'nouns': okt_nouns})\n",
    "    data.append({'analyzer': 'mecab', 'morphs': mecab_morphs, 'nouns': mecab_nouns})\n",
    "    data.append({'analyzer': 'kiwi', 'morphs': kiwi_morphs, 'nouns': kiwi_nouns})\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "747f666b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>morphs</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>okt</td>\n",
       "      <td>[삼, 성, 전자, 가, 새로운, 스마트폰, 을, 출시, 했습니다]</td>\n",
       "      <td>[전자, 스마트폰, 출시]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mecab</td>\n",
       "      <td>[삼성전자, 가, 새로운, 스마트폰, 을, 출시, 했, 습니다]</td>\n",
       "      <td>[삼성전자, 스마트폰, 출시]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kiwi</td>\n",
       "      <td>[삼성전자, 가, 새롭, 은, 스마트폰, 을, 출시, 하, 었, 습니다]</td>\n",
       "      <td>[스마트폰, 출시]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer                                    morphs             nouns\n",
       "0      okt     [삼, 성, 전자, 가, 새로운, 스마트폰, 을, 출시, 했습니다]    [전자, 스마트폰, 출시]\n",
       "1    mecab       [삼성전자, 가, 새로운, 스마트폰, 을, 출시, 했, 습니다]  [삼성전자, 스마트폰, 출시]\n",
       "2     kiwi  [삼성전자, 가, 새롭, 은, 스마트폰, 을, 출시, 하, 었, 습니다]        [스마트폰, 출시]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"삼성전자가 새로운 스마트폰을 출시했습니다\"\n",
    "df = compare_analyze(text)\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b63b5379",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "한국어 텍스트 전처리 (Kiwi 사용):\n",
    "1. 특수문자 제거(한글, 공백만 유지)\n",
    "2. Kiwi 형태소 분석\n",
    "3. pos_tags가 지정되면 해당 품사만 추출 (예: ['NNG', 'NNP', 'VA', 'VV'])\n",
    "   지정 안되면 전체 토큰 반환\n",
    "4. min_length 미만 길이의 토큰 제거\n",
    "5. 공백으로 연결하여 문자열 반환"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3598b168",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# a : 42   A : 78    가나다...\n",
    "# b : 43   B : 79\n",
    "# c : 44\n",
    "# ...\n",
    "# z : 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "091e7dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"이 마스크 정말 좋아요!! 사이즈도 딱 맞고 가격도 착해요 ㅎㅎ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6ed8ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 마스크 정말 좋아요 사이즈도 딱 맞고 가격도 착해요\n",
      "['마스크', '정말', '어요', '사이즈', '가격', '착하', '어요']\n"
     ]
    }
   ],
   "source": [
    "# text = \"%^&&*@@$%^\"\n",
    "text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "text = re.sub(r'\\s+', ' ', text).strip()\n",
    "print(text)\n",
    "tokens = kiwi.tokenize(text)\n",
    "tokens\n",
    "\n",
    "words = [t.form for t in tokens if len(t.form) >= 2]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64e3ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_korean(text, pos_tags=None, min_length=2):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "\n",
    "    cleaned = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]\", \" \", str(text))\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    if not cleaned:\n",
    "        return \"\"\n",
    "\n",
    "    tokens = kiwi.tokenize(cleaned)\n",
    "\n",
    "    if isinstance(pos_tags, str):\n",
    "        pos_tags = [pos_tags]\n",
    "    pos_set = set(pos_tags) if pos_tags else None\n",
    "\n",
    "    filtered = [\n",
    "        t.form for t in tokens\n",
    "        if (pos_set is None or t.tag in pos_set) and len(t.form) >= min_length\n",
    "    ]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "051367ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_korean(text, pos_tags=None, min_length=2):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]\", \" \", str(text))\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    \n",
    "    if pos_tags:\n",
    "        words = [t.form for t in tokens if t.tag in pos_tags and len(t.form) >= min_length]\n",
    "    else:\n",
    "        words = [t.form for t in tokens if len(t.form) > min_length]\n",
    "    \n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d8207b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"이 마스크 정말 좋아요!! 사이즈도 딱 맞고 가격도 착해요 ㅎㅎ\",\n",
    "    \"배송은 빨랐는데 품질이 별로예요... 얇고 냄새나요ㅠㅠ\",\n",
    "    \"가성비 최고! 10개 묶음으로 샀는데 넉넉하게 쓸 수 있어요\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e1958212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마스크 사이즈 가격 \n",
      "\n",
      "배송 품질 냄새 \n",
      "\n",
      "가성비 최고 묶음 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in reviews:\n",
    "    print(preprocess_korean(r, pos_tags=['NNG', 'NNP']), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1f72798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    \"자연어처리는 인공지능의 핵심 분야입니다\",\n",
    "    \"딥러닝을 활용한 자연어처리 기술이 발전하고 있습니다\",\n",
    "    \"텍스트 데이터를 벡터로 변환하는 것이 임베딩입니다\",\n",
    "    \"워드투벡 모델은 단어를 벡터 공간에 매핑합니다\",\n",
    "    \"한국어 자연어처리에는 형태소 분석이 중요합니다\",\n",
    "    \"트랜스포머 모델은 어텐션 메커니즘을 사용합니다\",\n",
    "    \"BERT는 양방향 사전학습 모델입니다\",\n",
    "    \"임베딩은 단어의 의미를 수치로 표현합니다\",\n",
    "] * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "51560cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./content/spm_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for t in sample_texts:\n",
    "        f.write(t + '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1de2470b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\"안녕하세요\"\n",
    "\"안녕하세요, 안녕히 계세요\"\n",
    "\"안녕히 계세요\"\n",
    "\n",
    "resize, padding\n",
    "128\n",
    "[안녕하세요 0,0,0, 0, ......]\n",
    "300 -> 128\n",
    "\n",
    "안녕, 하세, 안녕히,,,, 1\n",
    "\n",
    "잘\n",
    "\"안녕하세요, 안녕히 계세요\" => [안녕, 하세요, 안녕, 히, 계세요]\n",
    "[2,5, 56, .. 1, 105,       3, 0,0,0,0,0, 0] -> (x, x,)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1431a205",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "{가 : 3\n",
    " 간다 : 4\n",
    " 안녕 : 5\n",
    " 학교 : 6\n",
    "... 하세요 : 56\n",
    "계세요 : 105 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e872bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input = './content/spm_train.txt',\n",
    "    model_prefix = './content/spm_demo',\n",
    "vocab_size=100,\n",
    "    pad_id=0 ,unk_id=1, bos_id=2, eos_id=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a19bacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='./content/spm_demo.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c5d68062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9fa6514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"자연어처리 기술을 배워봅시다.\",\n",
    "    \"딥러닝과 임베딩의 관계를 이해합시다\",\n",
    "    \"새로운문장도잘분리됩니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "115263a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁자연어처리', '▁', '기', '술', '을', '▁', '배', '워', '봅시', '다', '.']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_tokens = sp.encode(\"자연어처리 기술을 배워봅시다.\", out_type=str)\n",
    "str_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "35a44e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 4, 46, 61, 24, 4, 1, 87, 1, 7, 6]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_tokens = sp.encode(\"자연어처리 기술을 배워봅시다.\", out_type=int)\n",
    "int_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "72ddcecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.bos_id(), sp.eos_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9776c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : 자연어처리 기술을 배워봅시다.\n",
      "문자열 : ['▁자연어처리', '▁', '기', '술', '을', '▁', '배', '워', '봅시', '다', '.']\n",
      "정수 : [16, 4, 46, 61, 24, 4, 1, 87, 1, 7, 6]\n",
      "BOS/EOS : [2, 16, 4, 46, 61, 24, 4, 1, 87, 1, 7, 6, 3] \n",
      "\n",
      "원문 : 딥러닝과 임베딩의 관계를 이해합시다\n",
      "문자열 : ['▁', '딥', '러', '닝', '과', '▁', '임', '베', '딩', '의', '▁', '관계', '를', '▁', '이', '해', '합', '시', '다']\n",
      "정수 : [4, 49, 51, 48, 1, 4, 88, 89, 90, 13, 4, 1, 11, 4, 8, 1, 10, 1, 7]\n",
      "BOS/EOS : [2, 4, 49, 51, 48, 1, 4, 88, 89, 90, 13, 4, 1, 11, 4, 8, 1, 10, 1, 7, 3] \n",
      "\n",
      "원문 : 새로운문장도잘분리됩니다.\n",
      "문자열 : ['▁', '새', '로', '운문장도잘', '분', '리', '됩', '니', '다', '.']\n",
      "정수 : [4, 1, 18, 1, 97, 96, 1, 5, 7, 6]\n",
      "BOS/EOS : [2, 4, 1, 18, 1, 97, 96, 1, 5, 7, 6, 3] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in test_sentences:\n",
    "    str_tokens = sp.encode(sent, out_type=str)\n",
    "    int_tokens = sp.encode(sent, out_type=int)\n",
    "    print(f'원문 : {sent}')\n",
    "    print(f'문자열 : {str_tokens}')\n",
    "    print(f'정수 : {int_tokens}')\n",
    "    print(f'BOS/EOS : {[sp.bos_id()] + int_tokens + [sp.eos_id()]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "62b127df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"자연어처리 기술을 배워봅시다\",\n",
    "    \"딥러닝과 임베딩의 관계를 이해합시다\",\n",
    "    \"새로운문장도잘분리됩니다\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "799039c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 자연어처리 기술을 배워봅시다\n",
      "1 딥러닝과 임베딩의 관계를 이해합시다\n",
      "2 새로운문장도잘분리됩니다\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(test_sentences):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5452ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(texts, sp_model, max_len=16):\n",
    "    pad_id = sp_model.pad_id() if hasattr(sp_model, 'pad_id') else 0\n",
    "    bos = sp_model.bos_id() if hasattr(sp_model, 'bos_id') else -1\n",
    "    eos = sp_model.eos_id() if hasattr(sp_model, 'eos_id') else -1\n",
    "    results = []\n",
    "    masks = []\n",
    "    for t in texts:\n",
    "        ids = sp_model.encode(t, out_type=int)\n",
    "        seq = []\n",
    "        if bos is not None and bos >= 0:\n",
    "            seq.append(bos)\n",
    "        seq += ids\n",
    "        if eos is not None and eos >= 0:\n",
    "            seq.append(eos)\n",
    "        if len(seq) > max_len:\n",
    "            seq = seq[:max_len]\n",
    "        mask = [1]*len(seq)\n",
    "        if len(seq) < max_len:\n",
    "            pad_len = max_len - len(seq)\n",
    "            seq = seq + [pad_id]*pad_len\n",
    "            mask = mask + [0]*pad_len\n",
    "        results.append(seq)\n",
    "        masks.append(mask)\n",
    "    return results, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dd8a4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(texts, sp_model, max_len = 16):\n",
    "    result = np.zeros((len(texts), max_len), dtype= np.int64)\n",
    "    for i, text in enumerate(texts):\n",
    "        ids = sp_model.encode(text, out_type=int)\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "\n",
    "        result[i, : len(ids)] = ids\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3dd331b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  4, 46, 61, 24,  4,  1, 87,  1,  7,  6,  0,  0,  0,  0,  0],\n",
       "       [ 4, 49, 51, 48,  1,  4, 88, 89, 90, 13,  4,  1, 11,  4,  8,  1],\n",
       "       [ 4,  1, 18,  1, 97, 96,  1,  5,  7,  6,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_and_pad(test_sentences, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d936c7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.zeros((len(\"자연어처리 기술을 배워봅시다.\"), 16), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "28f7e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization: 모델 경량화\n",
    "# float32 : floating point 32 byte -> int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eb57ec93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "texts = [\"자연어 처리를 배웁니다.\", \"임베딩이 중요합니다\", \"딥러닝\"]\n",
    "result = encode_and_pad(texts, sp, max_len=10)\n",
    "print(f'결과 shape: {result.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5d8171aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 93, 94, 21,  4, 92, 96, 11,  4,  1],\n",
       "       [ 4, 88, 89, 90,  8,  4, 38, 65, 10,  5],\n",
       "       [ 4, 49, 51, 48,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "691eea3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((len(\"자연어처리 기술을 배워봅시다.\"), 16, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
